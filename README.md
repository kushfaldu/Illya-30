# Illya 30

This repository collects and explores the "Illya 30"—the top 30 deep learning papers recommended by Ilya Sutskever (co-founder of OpenAI and a leading figure in AI research). According to Sutskever, mastering these papers will give you a solid understanding of the core ideas that drive modern AI.

## What is the "Illya 30"?

The "Illya 30" is a curated reading list of approximately 30 research papers and resources that cover the most important breakthroughs and concepts in deep learning, as recommended by Ilya Sutskever. The list is widely shared in the AI community and is considered an essential roadmap for anyone serious about understanding deep learning.

## Papers and Topics Included

The list covers a wide range of topics, including:
- Neural network architectures (CNNs, RNNs, LSTMs, Transformers)
- Regularization and optimization techniques
- Attention mechanisms
- Model scaling and efficiency
- Foundational theory (Kolmogorov complexity, MDL principle)
- Applications in vision, language, and more

### The Illya 30: Full Paper List

1. [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) (Sasha Rush, et al.)
2. [The First Law of Complexodynamics](https://www.scottaaronson.com/blog/?p=2756) (Scott Aaronson)
3. [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) (Andrej Karpathy)
4. [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) (Christopher Olah)
5. [Recurrent Neural Network Regularization](https://arxiv.org/abs/1409.2329) (Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals)
6. [Keeping Neural Networks Simple by Minimizing the Description Length of the Weights](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf) (Geoffrey E. Hinton, Drew van Camp)
7. [Pointer Networks](https://arxiv.org/abs/1506.03134) (Oriol Vinyals, Meire Fortunato, Navdeep Jaitly)
8. [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) (Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton)
9. [Order Matters: Sequence to Sequence for Sets](https://arxiv.org/abs/1511.06391) (Oriol Vinyals, Samy Bengio, Manjunath Kudlur)
10. [GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism](https://arxiv.org/abs/1811.06965) (Yanping Huang, et al.)
11. [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) (Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun)
12. [Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122) (Fisher Yu, Vladlen Koltun)
13. [Neural Message Passing for Quantum Chemistry](https://arxiv.org/abs/1704.01212) (Justin Gilmer, et al.)
14. [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Ashish Vaswani, et al.)
15. [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) (Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio)
16. [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027) (Kaiming He, et al.)
17. [A Simple Neural Network Module for Relational Reasoning](https://arxiv.org/abs/1706.01427) (Adam Santoro, et al.)
18. [Variational Lossy Autoencoder](https://arxiv.org/abs/1611.02731) (Xi Chen, et al.)
19. [Relational Recurrent Neural Networks](https://arxiv.org/abs/1806.01822) (Adam Santoro, et al.)
20. [Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton](https://arxiv.org/abs/1405.6903) (Scott Aaronson, Sean M. Carroll, Lauren Ouellette)
21. [Neural Turing Machines](https://arxiv.org/abs/1410.5401) (Alex Graves, Greg Wayne, Ivo Danihelka)
22. [Deep Speech 2: End-to-End Speech Recognition in English and Mandarin](https://arxiv.org/abs/1512.02595) (Dario Amodei, et al.)
23. [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) (Jared Kaplan, et al.)
24. [A Tutorial Introduction to the Minimum Description Length Principle](https://arxiv.org/abs/math/0406077) (Peter Grünwald)
25. [Machine Super Intelligence (PhD Thesis)](https://www.doc.ic.ac.uk/~sl203/thesis.pdf) (Shane Legg)
26. [Kolmogorov Complexity and Algorithmic Randomness](https://www.amazon.com/Kolmogorov-Complexity-Algorithmic-Randomness-Mathematics/dp/0821842625) (A. Shen, V. A. Uspensky, N. Vereshchagin)
27. [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)
28. [Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/abs/2309.17453) (Fabian Gloeckle, et al.)
29. [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) (Vladimir Karpukhin, et al.)
30. [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) (Patrick Lewis, et al.)

## How to Use This Repo

- **Read**: Work through the papers in the list (see [this example list](https://aman.ai/primers/ai/top-30-papers/) or [this GitHub repo](https://github.com/dzyim/ilya-sutskever-recommended-reading)).
- **Summarize**: Add your own notes, summaries, or code implementations for each paper.
- **Discuss**: Use issues or discussions to talk about insights, questions, or applications.

## Why Study the Illya 30?

- Build a strong foundation in deep learning theory and practice.
- Understand the historical context and evolution of key ideas.
- Prepare for research or advanced engineering roles in AI.

## Resources

- [Ilya Sutskever's Top 30 Reading List (Aman.ai)](https://aman.ai/primers/ai/top-30-papers/)
- [GitHub: ilya-sutskever-recommended-reading](https://github.com/dzyim/ilya-sutskever-recommended-reading)
- [AI Connections: Ilya Sutskever's Reading List](https://aiconnections.substack.com/p/ilya-sutskevers-reading-list)
